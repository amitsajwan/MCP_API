#!/usr/bin/env python3
"""
Enhanced MCP Client with Direct LLM Integration
This client assumes LLM access is always available and uses it for
intelligent argument validation and dynamic planning.
"""

import json
import logging
import os
import time
from typing import Dict, Any, List, Optional, Union
from dataclasses import dataclass, field
from datetime import datetime
import requests
from openai import AzureOpenAI
import yaml

logger = logging.getLogger(__name__)

@dataclass
class Tool:
    """Tool definition with LLM enhancement capabilities."""
    name: str
    description: str
    input_schema: Dict[str, Any]
    
    # LLM-enhanced fields
    usage_examples: List[str] = field(default_factory=list)
    common_patterns: List[str] = field(default_factory=list)
    validation_hints: List[str] = field(default_factory=list)

@dataclass
class ValidationResult:
    """Result of LLM-powered argument validation."""
    is_valid: bool
    cleaned_args: Dict[str, Any]
    suggestions: List[str] = field(default_factory=list)
    confidence: float = 1.0
    reasoning: str = ""
    auto_fixes: Dict[str, Any] = field(default_factory=dict)

@dataclass
class ExecutionPlan:
    """Dynamic execution plan generated by LLM."""
    tool_name: str
    arguments: Dict[str, Any]
    reasoning: str
    confidence: float
    alternatives: List[Dict[str, Any]] = field(default_factory=list)
    expected_outcome: str = ""

class EnhancedMCPClient:
    """Enhanced MCP Client with direct LLM integration for intelligent operations."""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # Server connection
        self.base_url = config.get('server_url', 'http://localhost:8000')
        self.session = requests.Session()
        
        # Authentication
        self.auth_token = None
        self.auth_config = config.get('auth', {})
        
        # Initialize LLM client (assuming always available)
        self._init_llm_client()
        
        # Tool registry
        self.tools: Dict[str, Tool] = {}
        
        # Learning and adaptation
        self.execution_history: List[Dict[str, Any]] = []
        self.learned_patterns: Dict[str, List[str]] = {}
        
        logger.info("üöÄ Enhanced MCP Client initialized with LLM capabilities")
    
    def _init_llm_client(self):
        """Initialize LLM client - assumes always available."""
        azure_config = self.config.get('azure_openai', {})
        self.llm_client = AzureOpenAI(
            azure_endpoint=azure_config.get('endpoint') or os.getenv("AZURE_OPENAI_ENDPOINT"),
            api_key=azure_config.get('api_key') or os.getenv("AZURE_OPENAI_API_KEY"),
            api_version="2024-02-15-preview"
        )
        self.deployment_name = azure_config.get('deployment', 'gpt-4')
        logger.info("‚úÖ LLM client initialized successfully")
    
    def authenticate(self, username: str = None, password: str = None) -> bool:
        """Authenticate with the MCP server."""
        try:
            # Use provided credentials or config
            auth_username = username or self.auth_config.get('username')
            auth_password = password or self.auth_config.get('password')
            
            if not auth_username or not auth_password:
                logger.error("‚ùå Authentication credentials not provided")
                return False
            
            # Login request
            login_data = {
                "username": auth_username,
                "password": auth_password
            }
            
            response = self.session.post(
                f"{self.base_url}/auth/login",
                json=login_data,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                self.auth_token = result.get('access_token')
                
                # Set authorization header for future requests
                self.session.headers.update({
                    'Authorization': f'Bearer {self.auth_token}'
                })
                
                logger.info("‚úÖ Authentication successful")
                return True
            else:
                logger.error(f"‚ùå Authentication failed: {response.status_code} - {response.text}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Authentication error: {e}")
            return False
    
    def discover_tools(self) -> bool:
        """Discover available tools from the server with LLM enhancement."""
        try:
            response = self.session.get(f"{self.base_url}/tools", timeout=30)
            
            if response.status_code == 200:
                tools_data = response.json()
                
                for tool_data in tools_data.get('tools', []):
                    tool = Tool(
                        name=tool_data['name'],
                        description=tool_data['description'],
                        input_schema=tool_data.get('inputSchema', {})
                    )
                    
                    # Enhance tool with LLM insights
                    self._enhance_tool_with_llm(tool)
                    
                    self.tools[tool.name] = tool
                
                logger.info(f"‚úÖ Discovered {len(self.tools)} tools with LLM enhancements")
                return True
            else:
                logger.error(f"‚ùå Failed to discover tools: {response.status_code}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Tool discovery error: {e}")
            return False
    
    def _enhance_tool_with_llm(self, tool: Tool):
        """Enhance tool definition with LLM insights."""
        try:
            prompt = f"""
Analyze this API tool and provide insights:

**Tool Name:** {tool.name}
**Description:** {tool.description}
**Input Schema:** {json.dumps(tool.input_schema, indent=2)}

Provide:
1. 3-5 usage examples (brief descriptions)
2. 3-5 common usage patterns
3. 3-5 validation hints for parameters

Format as JSON:
{{
  "usage_examples": ["example1", "example2", ...],
  "common_patterns": ["pattern1", "pattern2", ...],
  "validation_hints": ["hint1", "hint2", ...]
}}
"""
            
            response = self.llm_client.chat.completions.create(
                model=self.deployment_name,
                messages=[
                    {"role": "system", "content": "You are an API analysis expert. Provide practical insights for tool usage."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=500
            )
            
            insights = json.loads(response.choices[0].message.content)
            
            tool.usage_examples = insights.get('usage_examples', [])
            tool.common_patterns = insights.get('common_patterns', [])
            tool.validation_hints = insights.get('validation_hints', [])
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to enhance tool {tool.name} with LLM: {e}")
    
    def validate_arguments_with_llm(self, tool_name: str, arguments: Dict[str, Any]) -> ValidationResult:
        """Use LLM to intelligently validate and enhance arguments."""
        if tool_name not in self.tools:
            return ValidationResult(
                is_valid=False,
                cleaned_args=arguments,
                suggestions=[f"Tool '{tool_name}' not found"]
            )
        
        tool = self.tools[tool_name]
        
        try:
            prompt = f"""
Validate and enhance these API arguments:

**Tool:** {tool_name}
**Description:** {tool.description}
**Schema:** {json.dumps(tool.input_schema, indent=2)}
**Arguments:** {json.dumps(arguments, indent=2)}
**Validation Hints:** {tool.validation_hints}

Analyze the arguments and provide:
1. Are they valid? (true/false)
2. Cleaned/enhanced arguments (fix types, add defaults, etc.)
3. Suggestions for improvement
4. Confidence score (0.0-1.0)
5. Reasoning for validation decision
6. Auto-fixes applied

Format as JSON:
{{
  "is_valid": true/false,
  "cleaned_args": {{...}},
  "suggestions": ["suggestion1", ...],
  "confidence": 0.95,
  "reasoning": "explanation",
  "auto_fixes": {{"field": "fix_description"}}
}}
"""
            
            response = self.llm_client.chat.completions.create(
                model=self.deployment_name,
                messages=[
                    {"role": "system", "content": "You are an expert API argument validator. Provide thorough validation with helpful suggestions."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
                max_tokens=800
            )
            
            result_data = json.loads(response.choices[0].message.content)
            
            return ValidationResult(
                is_valid=result_data.get('is_valid', False),
                cleaned_args=result_data.get('cleaned_args', arguments),
                suggestions=result_data.get('suggestions', []),
                confidence=result_data.get('confidence', 0.5),
                reasoning=result_data.get('reasoning', ''),
                auto_fixes=result_data.get('auto_fixes', {})
            )
            
        except Exception as e:
            logger.error(f"‚ùå LLM validation failed: {e}")
            return ValidationResult(
                is_valid=False,
                cleaned_args=arguments,
                suggestions=[f"Validation error: {str(e)}"]
            )
    
    def create_execution_plan(self, user_intent: str, context: Dict[str, Any] = None) -> ExecutionPlan:
        """Create dynamic execution plan based on user intent."""
        context = context or {}
        
        try:
            # Include available tools in context
            tools_info = {}
            for name, tool in self.tools.items():
                tools_info[name] = {
                    "description": tool.description,
                    "schema": tool.input_schema,
                    "examples": tool.usage_examples[:2],  # Include top examples
                    "patterns": tool.common_patterns[:2]
                }
            
            prompt = f"""
Create an execution plan for this user intent:

**User Intent:** {user_intent}
**Context:** {json.dumps(context, indent=2)}
**Available Tools:** {json.dumps(tools_info, indent=2)}
**Execution History:** {self.execution_history[-3:] if self.execution_history else []}

Analyze the intent and create an execution plan:
1. Which tool to use
2. What arguments to provide
3. Reasoning for the choice
4. Confidence in the plan
5. Alternative approaches
6. Expected outcome

Format as JSON:
{{
  "tool_name": "selected_tool",
  "arguments": {{...}},
  "reasoning": "why this approach",
  "confidence": 0.95,
  "alternatives": [
    {{"tool": "alt_tool", "args": {{...}}, "reason": "why alternative"}}
  ],
  "expected_outcome": "what to expect"
}}
"""
            
            response = self.llm_client.chat.completions.create(
                model=self.deployment_name,
                messages=[
                    {"role": "system", "content": "You are a dynamic execution planner. Create optimal plans for API tool usage based on user intent."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=1000
            )
            
            plan_data = json.loads(response.choices[0].message.content)
            
            return ExecutionPlan(
                tool_name=plan_data.get('tool_name', ''),
                arguments=plan_data.get('arguments', {}),
                reasoning=plan_data.get('reasoning', ''),
                confidence=plan_data.get('confidence', 0.5),
                alternatives=plan_data.get('alternatives', []),
                expected_outcome=plan_data.get('expected_outcome', '')
            )
            
        except Exception as e:
            logger.error(f"‚ùå Failed to create execution plan: {e}")
            return ExecutionPlan(
                tool_name="",
                arguments={},
                reasoning=f"Planning failed: {str(e)}",
                confidence=0.0
            )
    
    def execute_tool(self, tool_name: str, arguments: Dict[str, Any], 
                    validate: bool = True) -> Dict[str, Any]:
        """Execute a tool with intelligent validation and error handling."""
        start_time = time.time()
        
        try:
            # Validate arguments with LLM if requested
            if validate:
                validation = self.validate_arguments_with_llm(tool_name, arguments)
                
                if not validation.is_valid:
                    logger.warning(f"‚ö†Ô∏è Validation failed for {tool_name}: {validation.reasoning}")
                    return {
                        "success": False,
                        "error": "Validation failed",
                        "validation_result": {
                            "suggestions": validation.suggestions,
                            "reasoning": validation.reasoning
                        }
                    }
                
                # Use cleaned arguments
                arguments = validation.cleaned_args
                
                if validation.auto_fixes:
                    logger.info(f"üîß Applied auto-fixes: {validation.auto_fixes}")
            
            # Execute the tool
            response = self.session.post(
                f"{self.base_url}/tools/{tool_name}/call",
                json={"arguments": arguments},
                timeout=60
            )
            
            execution_time = time.time() - start_time
            
            if response.status_code == 200:
                result = response.json()
                
                # Record successful execution
                self._record_execution(tool_name, arguments, result, execution_time, True)
                
                logger.info(f"‚úÖ Tool {tool_name} executed successfully in {execution_time:.2f}s")
                return {
                    "success": True,
                    "result": result,
                    "execution_time": execution_time
                }
            else:
                error_msg = f"Tool execution failed: {response.status_code} - {response.text}"
                
                # Record failed execution
                self._record_execution(tool_name, arguments, {"error": error_msg}, execution_time, False)
                
                # Try to get intelligent error analysis
                error_analysis = self._analyze_error_with_llm(tool_name, arguments, response.text)
                
                logger.error(f"‚ùå {error_msg}")
                return {
                    "success": False,
                    "error": error_msg,
                    "error_analysis": error_analysis,
                    "execution_time": execution_time
                }
                
        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = f"Tool execution exception: {str(e)}"
            
            # Record failed execution
            self._record_execution(tool_name, arguments, {"error": error_msg}, execution_time, False)
            
            logger.error(f"‚ùå {error_msg}")
            return {
                "success": False,
                "error": error_msg,
                "execution_time": execution_time
            }
    
    def execute_with_intent(self, user_intent: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """Execute based on user intent using dynamic planning."""
        logger.info(f"üéØ Executing intent: {user_intent}")
        
        # Create execution plan
        plan = self.create_execution_plan(user_intent, context)
        
        if not plan.tool_name:
            return {
                "success": False,
                "error": "Could not create execution plan",
                "reasoning": plan.reasoning
            }
        
        logger.info(f"üìã Execution plan: {plan.tool_name} (confidence: {plan.confidence:.2f})")
        logger.info(f"üí≠ Reasoning: {plan.reasoning}")
        
        # Execute the planned tool
        result = self.execute_tool(plan.tool_name, plan.arguments)
        
        # If execution failed and we have alternatives, try them
        if not result.get("success") and plan.alternatives:
            logger.info("üîÑ Trying alternative approaches...")
            
            for alt in plan.alternatives[:2]:  # Try up to 2 alternatives
                logger.info(f"üîÑ Trying alternative: {alt.get('tool')} - {alt.get('reason')}")
                alt_result = self.execute_tool(alt.get('tool', ''), alt.get('args', {}))
                
                if alt_result.get("success"):
                    logger.info("‚úÖ Alternative approach succeeded")
                    return {
                        "success": True,
                        "result": alt_result["result"],
                        "execution_plan": plan,
                        "used_alternative": True,
                        "alternative_used": alt
                    }
        
        return {
            "success": result.get("success", False),
            "result": result.get("result"),
            "error": result.get("error"),
            "execution_plan": plan,
            "used_alternative": False
        }
    
    def _analyze_error_with_llm(self, tool_name: str, arguments: Dict[str, Any], 
                               error_message: str) -> Dict[str, Any]:
        """Analyze errors using LLM to provide intelligent suggestions."""
        try:
            tool = self.tools.get(tool_name)
            
            prompt = f"""
Analyze this API execution error and provide helpful insights:

**Tool:** {tool_name}
**Arguments:** {json.dumps(arguments, indent=2)}
**Error Message:** {error_message}
**Tool Schema:** {json.dumps(tool.input_schema if tool else {}, indent=2)}
**Recent Executions:** {self.execution_history[-3:] if self.execution_history else []}

Provide:
1. Likely cause of the error
2. Specific suggestions to fix it
3. Alternative approaches
4. Whether this is a common issue

Format as JSON:
{{
  "likely_cause": "explanation",
  "suggestions": ["fix1", "fix2", ...],
  "alternatives": ["approach1", "approach2", ...],
  "is_common": true/false,
  "severity": "low/medium/high"
}}
"""
            
            response = self.llm_client.chat.completions.create(
                model=self.deployment_name,
                messages=[
                    {"role": "system", "content": "You are an API error analysis expert. Provide actionable insights for fixing errors."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
                max_tokens=600
            )
            
            return json.loads(response.choices[0].message.content)
            
        except Exception as e:
            logger.error(f"Failed to analyze error with LLM: {e}")
            return {
                "likely_cause": "Unknown error",
                "suggestions": ["Check the error message and tool documentation"],
                "alternatives": [],
                "is_common": False,
                "severity": "medium"
            }
    
    def _record_execution(self, tool_name: str, arguments: Dict[str, Any], 
                         result: Dict[str, Any], execution_time: float, success: bool):
        """Record execution for learning and pattern recognition."""
        execution_record = {
            "timestamp": datetime.now().isoformat(),
            "tool_name": tool_name,
            "arguments": arguments,
            "result": result,
            "execution_time": execution_time,
            "success": success
        }
        
        self.execution_history.append(execution_record)
        
        # Keep only recent executions
        if len(self.execution_history) > 100:
            self.execution_history = self.execution_history[-100:]
        
        # Learn patterns from successful executions
        if success:
            if tool_name not in self.learned_patterns:
                self.learned_patterns[tool_name] = []
            
            pattern = f"args:{len(arguments)}_time:{execution_time:.1f}s"
            if pattern not in self.learned_patterns[tool_name]:
                self.learned_patterns[tool_name].append(pattern)
    
    def get_tool_suggestions(self, partial_intent: str) -> List[Dict[str, Any]]:
        """Get intelligent tool suggestions based on partial user intent."""
        try:
            prompt = f"""
Suggest the most relevant tools for this partial user intent:

**Partial Intent:** {partial_intent}
**Available Tools:** {list(self.tools.keys())}
**Tool Descriptions:** {[(name, tool.description) for name, tool in self.tools.items()]}
**Usage Patterns:** {self.learned_patterns}

Suggest up to 5 most relevant tools with:
1. Tool name
2. Relevance score (0.0-1.0)
3. Reason for suggestion
4. Likely arguments needed

Format as JSON array:
[
  {
    "tool_name": "tool1",
    "relevance": 0.95,
    "reason": "why relevant",
    "likely_args": {"arg1": "value1"}
  }
]
"""
            
            response = self.llm_client.chat.completions.create(
                model=self.deployment_name,
                messages=[
                    {"role": "system", "content": "You are a tool recommendation expert. Suggest the most relevant tools for user intents."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=800
            )
            
            suggestions = json.loads(response.choices[0].message.content)
            return suggestions if isinstance(suggestions, list) else []
            
        except Exception as e:
            logger.error(f"Failed to get tool suggestions: {e}")
            return []
    
    def get_execution_insights(self) -> Dict[str, Any]:
        """Get insights about tool usage patterns and performance."""
        if not self.execution_history:
            return {"message": "No execution history available"}
        
        total_executions = len(self.execution_history)
        successful_executions = sum(1 for ex in self.execution_history if ex["success"])
        success_rate = successful_executions / total_executions if total_executions > 0 else 0
        
        # Tool usage frequency
        tool_usage = {}
        total_time = 0
        
        for execution in self.execution_history:
            tool_name = execution["tool_name"]
            exec_time = execution["execution_time"]
            
            if tool_name not in tool_usage:
                tool_usage[tool_name] = {"count": 0, "total_time": 0, "successes": 0}
            
            tool_usage[tool_name]["count"] += 1
            tool_usage[tool_name]["total_time"] += exec_time
            total_time += exec_time
            
            if execution["success"]:
                tool_usage[tool_name]["successes"] += 1
        
        # Calculate averages and success rates
        for tool_name, stats in tool_usage.items():
            stats["avg_time"] = stats["total_time"] / stats["count"]
            stats["success_rate"] = stats["successes"] / stats["count"]
        
        return {
            "total_executions": total_executions,
            "success_rate": success_rate,
            "total_execution_time": total_time,
            "avg_execution_time": total_time / total_executions if total_executions > 0 else 0,
            "tool_usage": tool_usage,
            "learned_patterns": self.learned_patterns,
            "most_used_tools": sorted(tool_usage.items(), key=lambda x: x[1]["count"], reverse=True)[:5],
            "fastest_tools": sorted(tool_usage.items(), key=lambda x: x[1]["avg_time"])[:5]
        }
    
    def close(self):
        """Clean up resources."""
        if hasattr(self, 'session'):
            self.session.close()
        logger.info("üîí Enhanced MCP Client closed")

# Example usage
if __name__ == "__main__":
    # Configuration
    config = {
        "server_url": "http://localhost:8000",
        "auth": {
            "username": "admin",
            "password": "password123"
        },
        "azure_openai": {
            "endpoint": os.getenv("AZURE_OPENAI_ENDPOINT"),
            "api_key": os.getenv("AZURE_OPENAI_API_KEY"),
            "deployment": "gpt-4"
        }
    }
    
    # Create enhanced client
    client = EnhancedMCPClient(config)
    
    try:
        # Authenticate
        if client.authenticate():
            print("‚úÖ Authentication successful")
            
            # Discover tools
            if client.discover_tools():
                print(f"‚úÖ Discovered {len(client.tools)} tools")
                
                # Example: Execute with intent
                result = client.execute_with_intent(
                    "I want to create a payment for $100 USD",
                    context={"user_id": "12345", "account": "premium"}
                )
                
                if result["success"]:
                    print("‚úÖ Intent execution successful:")
                    print(json.dumps(result["result"], indent=2))
                else:
                    print(f"‚ùå Intent execution failed: {result.get('error')}")
                
                # Example: Get tool suggestions
                suggestions = client.get_tool_suggestions("I need to check my balance")
                print(f"\nüí° Tool suggestions: {len(suggestions)}")
                for suggestion in suggestions:
                    print(f"  - {suggestion['tool_name']} (relevance: {suggestion['relevance']:.2f})")
                    print(f"    Reason: {suggestion['reason']}")
                
                # Example: Direct tool execution with validation
                if "createPayment" in client.tools:
                    result = client.execute_tool(
                        "createPayment",
                        {
                            "amount": "100.50",  # String that should be converted to number
                            "currency": "USD",
                            "description": "Test payment"
                        },
                        validate=True
                    )
                    
                    if result["success"]:
                        print("\n‚úÖ Direct tool execution successful")
                    else:
                        print(f"\n‚ùå Direct tool execution failed: {result.get('error')}")
                        if "error_analysis" in result:
                            print(f"Analysis: {result['error_analysis']}")
                
                # Show execution insights
                insights = client.get_execution_insights()
                print(f"\nüìä Execution Insights:")
                print(f"  - Total executions: {insights.get('total_executions', 0)}")
                print(f"  - Success rate: {insights.get('success_rate', 0):.2%}")
                print(f"  - Average execution time: {insights.get('avg_execution_time', 0):.2f}s")
                
            else:
                print("‚ùå Failed to discover tools")
        else:
            print("‚ùå Authentication failed")
            
    finally:
        client.close()